<h1 align="center">DPDFNet: Boosting DeepFilterNet2 via Dual-Path RNN</h1>
<br></br>

<div align="center">

[![Project Page](https://img.shields.io/badge/Project-Page-orange)](https://ceva-ip.github.io/DPDFNet/)
[![arXiv Paper](https://img.shields.io/badge/arXiv-Paper-b31b1b)](https://arxiv.org/abs/2512.16420)
[![ü§ó Hugging Face Models](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-yellow)](https://huggingface.co/Ceva-IP/DPDFNet)
[![ü§ó Hugging Face Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-yellowgreen)](https://huggingface.co/datasets/Ceva-IP/DPDFNet_EvalSet)

</div>

<p align="center">
 <sub><em><strong>--- Official implementation for the DPDFNet paper (2025) ---</strong></em></sub>
</p>

## Abstract
We present DPDFNet, a causal single-channel speech enhancement model that extends the DeepFilterNet2 architecture with dual-path blocks in the encoder, strengthening long-range temporal and cross-band modeling while preserving the original enhancement framework. In addition, we demonstrate that adding a loss component to mitigate over-attenuation in the enhanced speech, combined with a fine-tuning phase tailored for ‚Äúalways-on‚Äù applications, leads to substantial improvements in overall model performance. To compare our proposed architecture with a variety of causal open-source models, we created a new evaluation set comprising long, low-SNR recordings in 12 languages across everyday noise scenarios, better reflecting real-world conditions than commonly used benchmarks. On this evaluation set, DPDFNet delivers superior performance to other causal open-source models, including some that are substantially larger and more computationally demanding. We also propose a holistic metric named PRISM, a composite, scale-normalized aggregate of intrusive and non-intrusive metrics, which demonstrates clear scalability with the number of dual-path blocks. We further demonstrate on-device feasibility by deploying DPDFNet on Ceva-NeuPro‚Ñ¢-Nano edge NPUs. Results indicate that DPDFNet-4, our second-largest model, achieves real-time performance on NPN32 and runs even faster on NPN64, confirming that state-of-the-art quality can be sustained within strict embedded power and latency constraints.

---

## Repository Overview

This repo includes:

- **Offline enhancement** for a folder of WAV files (`enhance.py`)
- A **real-time microphone demo** with live spectrograms and A/B playback (`real_time_demo.py`)
- **TFLite inference** script for streaming frame-by-frame enhancement (`run_tflite.py`)
- **ONNX export** scripts for stateless multi-frame and stateful single-frame models
- Pre-trained **PyTorch checkpoints**, **TFLite** models, and exported **ONNX** models under `model_zoo/`
### 16 kHz models


### Models trained on:
Training data: DNS4 (downsampled), MLS, MUSAN, and FSD50K.

All models are causal and low-latency, designed for streaming use cases such as telephony, conferencing, and embedded devices.

### TFLite models

Place the `.tflite` files under:
```
model_zoo/tflite/
```

Use model names **without** the `.tflite` suffix in scripts (e.g., `dpdfnet4`, not `dpdfnet4.tflite`).

Supported model files:

#### 16 kHz models

| Model           | Params [M] | MACs [G] | TFLite Size [MB] | Intended Use                    |
| --------------- | :--------: | :------: | :--------------: | ------------------------------- |
| baseline.tflite |    2.31    |   0.36   |        8.5       | Fastest / lowest resource usage |
| dpdfnet2.tflite |    2.49    |   1.35   |       10.7       | Real-time / embedded devices    |
| dpdfnet4.tflite |    2.84    |   2.36   |       12.9       | Balanced performance            |
| dpdfnet8.tflite |    3.54    |   4.37   |       17.2       | Best enhancement quality        |

#### 48 kHz model

| Model                    | Params [M] | MACs [G] | TFLite Size [MB] | Intended Use                 |
| ------------------------ | :--------: | :------: | :--------------: | ---------------------------- |
| dpdfnet2_48khz_hr.tflite |    2.58    |   2.42   |       11.6       | High-resolution 48 kHz audio |

### Download models

Models are available on Hugging Face: https://huggingface.co/Ceva-IP/DPDFNet

Example download into the expected folder:
```bash
pip install -U "huggingface_hub[cli]"
mkdir -p model_zoo/tflite

# 16 kHz models
huggingface-cli download Ceva-IP/DPDFNet \
  baseline.tflite dpdfnet2.tflite dpdfnet4.tflite dpdfnet8.tflite \
  --local-dir model_zoo/tflite \
  --local-dir-use-symlinks False

# 48 kHz model
huggingface-cli download Ceva-IP/DPDFNet \
  dpdfnet2_48khz_hr.tflite \
  --local-dir model_zoo/tflite \
  --local-dir-use-symlinks False
```

---

## Quick start

### Install (full: offline + real-time demo)
```bash
python -m venv .venv
source .venv/bin/activate # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### Install (lightweight: offline enhancement only)
If you only need `enhance.py` and want to avoid heavy GUI/audio/demo dependencies:
```bash
pip install numpy scipy librosa soundfile tqdm tflite-runtime
```

Both scripts look for models under `./model_zoo/tflite` by default.

---

## Offline Enhancement

Enhance all `*.wav` files in a folder (**non-recursive**) and write enhanced WAVs to an output folder:

```bash
python enhance.py --noisy_dir /path/to/noisy_wavs --enhanced_dir /path/to/output --model_name dpdfnet8
```

What the script does:
- Loads audio (any sample rate), converts to **mono**, and resamples to the model's expected sample rate.
  - **16 kHz models:** `baseline`, `dpdfnet2`, `dpdfnet4`, `dpdfnet8`
  - **48 kHz model:** `dpdfnet2_48khz_hr`
- When using `dpdfnet2_48khz_hr`, the script automatically switches to a **48 kHz** processing pipeline.
- Runs the model **frame-by-frame in streaming mode**.
- Resamples back to the original sample rate, and saves **mono PCM_16 WAV** outputs.

Output filenames are created as:
```
<original_stem>_<model_name>.wav
```

---

## Real-Time Microphone Demo

![Real-time DPDFNet demo screen shot](figures/live_demo.png "Real-time DPDFNet demo screen shot")

The real-time demo performs streaming enhancement on microphone input, displays **Noisy vs Enhanced** live spectrograms, and lets you switch playback between the two.

### Run
```bash
python real_time_demo.py
```

### Configure
Edit constants near the top of `real_time_demo.py`:
- `MODEL_NAME`: `baseline | dpdfnet2 | dpdfnet4 | dpdfnet8 | dpdfnet2_48khz_hr`

### Usage
- Speak into your microphone.
- Use the UI buttons to switch playback:
 - **Noisy**: plays raw mic input
 - **Enhanced**: plays model output
- The console prints **ms per frame** to help assess real-time performance.

---

---

## TFLite Inference

`run_tflite.py` runs TFLite models on WAV files using streaming frame-by-frame inference.
Models are loaded from `model_zoo/tflite/`.

```bash
python run_tflite.py --noisy_dir /path/to/noisy_wavs --enhanced_dir /path/to/out --model_name dpdfnet8
```

Available `--model_name` options: `baseline`, `dpdfnet2`, `dpdfnet4`, `dpdfnet8`, `dpdfnet2_48khz_hr`.

When using `dpdfnet2_48khz_hr`, the script automatically switches to the 48kHz processing pipeline.

---

## ONNX Export

### Stateless (multi-frame)

Exports spectrum-in / spectrum-out models with dynamic time dimension:

```bash
python export_pytorch_to_onnx.py                          # all models
python export_pytorch_to_onnx.py --model_name dpdfnet2    # single model
```

Output: `model_zoo/stateless/<name>/model.onnx`

### Stateful (single-frame)

Exports single-frame models with explicit state I/O for real-time streaming:

```bash
python export_stateful_onnx.py                          # all models
python export_stateful_onnx.py --model_name dpdfnet2    # single model
```

Output: `model_zoo/stateful/<name>/model_stateful.onnx` + `initial_states.bin` + `initial_states.json`

---
## Metrics & Evaluation

To compute *intrusive* and *non-intrusive* metrics on our [DPDFNet EvalSet](https://huggingface.co/datasets/Ceva-IP/DPDFNet_EvalSet), we use the tools listed below. For aggregate quality reporting, we rely on PRISM, the scale‚Äënormalized composite metric introduced in the DPDFNet paper.

### Intrusive metrics: PESQ, STOI, SI-SNR
We provide a dedicated script, `pesq_stoi_sisnr_calc.py`, which computes **PESQ**, **STOI**, and **SI-SNR** for paired *reference* and *enhanced* audio. The script includes a built-in auto-alignment step that corrects small start-time offsets and drift between the reference and the enhanced signals before scoring, to ensure fair comparisons.

### Non-intrusive metrics
- **DNSMOS (P.835 & P.808)** - We use the **official** DNSMOS local inference script from the DNS Challenge repository: [`dnsmos_local.py`](https://github.com/microsoft/DNS-Challenge/blob/master/DNSMOS/dnsmos_local.py). Please follow their installation and model download instructions in that project before running. 
- **NISQA v2** - We use the **official** NISQA project: <https://github.com/gabrielmittag/NISQA>. Refer to their README for environment setup, pretrained model weights, and inference commands (*e.g.*, running `nisqa_predict.py` on a folder of WAVs).

## Citation
If you use this work, please cite the paper:

```bibtex
@article{rika2025dpdfnet,
 title = {DPDFNet: Boosting DeepFilterNet2 via Dual-Path RNN},
 author = {Rika, Daniel and Sapir, Nino and Gus, Ido},
 year = {2025},
}
```

## License

Apache License 2.0. See [LICENSE](LICENSE).